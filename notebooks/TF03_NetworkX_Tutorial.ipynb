{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-default",
   "metadata": {},
   "source": [
    "# NetworkX_Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-millennium",
   "metadata": {},
   "source": [
    "How to use networkX + loads of learning about network analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-charger",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import apiIntegrations.ga\n",
    "import topicmodelling.utilities.clean\n",
    "import topicmodelling.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-mining",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(r\"C:\\Users\\Tobias Fechner\\Documents\\1_Uni\\fyp\\git_repo_fyp\\data\\networktheory\\sample_data\\quakers_edgelist.csv\")\n",
    "nodes = pd.read_csv(r\"C:\\Users\\Tobias Fechner\\Documents\\1_Uni\\fyp\\git_repo_fyp\\data\\networktheory\\sample_data\\quakers_nodelist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-machinery",
   "metadata": {},
   "source": [
    "### Import and start using NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-chapel",
   "metadata": {},
   "source": [
    "Create a graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-makeup",
   "metadata": {},
   "source": [
    "Add the nodes and edges from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_nodes_from(nodes.loc[:, 'Name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edges_from(list(zip(edges.Source, edges.Target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-syntax",
   "metadata": {},
   "source": [
    "Print some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-capability",
   "metadata": {},
   "source": [
    "Now we add some attributes. These need to be a dictionary for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-material",
   "metadata": {},
   "source": [
    "Write a function to create attributes dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRowAttributes(row):\n",
    "    name = row.iloc[0]\n",
    "    \n",
    "    hist_sig = ('hist_sig', {name: row.iloc[1]})\n",
    "    gender = ('gender', {name: row.iloc[2]})\n",
    "    birth = ('birth', {name: row.iloc[3]})\n",
    "    death = ('death', {name: row.iloc[4]})\n",
    "    row_id = ('id', {name: row.iloc[5]})\n",
    "    \n",
    "    return hist_sig, gender, birth, death, row_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-religion",
   "metadata": {},
   "source": [
    "Apply to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['attributes'] = nodes.apply(lambda x: createRowAttributes(x), axis=1)\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-calculation",
   "metadata": {},
   "source": [
    "Apply function to add each attribute to the graph for each row in the dataframe. First create a function to add the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addAttributesToGraph(graph, attributes):\n",
    "    for attribute in attributes:\n",
    "        nx.set_node_attributes(graph, values=attribute[1], name=attribute[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-publicity",
   "metadata": {},
   "source": [
    "Then apply this function down the column to add each row's attributes to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-triumph",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes['attributes'].apply(lambda x: addAttributesToGraph(g, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-atlantic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in list(g.nodes())[:10]: # Loop through first 10 nodes, in our data \"n\" will be the name of the person\n",
    "    print(n, g.nodes[n]['birth']) # Access every node by its name, and then by the attribute \"birth\" - birth year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-orange",
   "metadata": {},
   "source": [
    "# Inspect the Network: Generate some Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-drill",
   "metadata": {},
   "source": [
    "### Structural Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-tender",
   "metadata": {},
   "source": [
    "These metrics look at the entire network as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-prairie",
   "metadata": {},
   "source": [
    "##### Network density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-restaurant",
   "metadata": {},
   "source": [
    "A good metric to begin with is network density. This is simply the ratio of actual edges in the network to all possible edges in the network. In an undirected network like this one, there could be a single edge between any two nodes, but as you saw in the visualization, only a few of those possible edges are actually present. Network density gives you a quick sense of how closely knit your network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(g)\n",
    "print(f\"Network density: {round(density, 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-corps",
   "metadata": {},
   "source": [
    "##### Shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-health",
   "metadata": {},
   "source": [
    "To calculate a shortest path, you’ll need to pass several input variables (information you give to a Python function): the whole graph, your source node, and your target node.\n",
    "\n",
    "Depending on the size of your network, this could take a little while to calculate, since Python first finds all possible paths and then picks the shortest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "fell_whitehead_path = nx.shortest_path(g, source=\"Margaret Fell\", target=\"George Whitehead\")\n",
    "\n",
    "print(\"Shortest path between Fell and Whitehead:\", fell_whitehead_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-paragraph",
   "metadata": {},
   "source": [
    "##### Diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-roman",
   "metadata": {},
   "source": [
    "There are many network metrics derived from shortest path lengths. One such measure is diameter, which is the longest of all shortest paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-dance",
   "metadata": {},
   "source": [
    "Since there is no shortest path between nodes of one component and nodes of another, nx.diameter() returns the “not connected” error. You can remedy this by first finding out if your Graph “is connected” (i.e. all one component) and, if not connected, finding the largest component and calculating diameter on that component alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your Graph has more than one component, this will return False:\n",
    "print(nx.is_connected(g))\n",
    "\n",
    "# Next, use nx.connected_components to get the list of components,\n",
    "# then use the max() command to find the largest one:\n",
    "components = nx.connected_components(g)\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "# Create a \"subgraph\" of just the largest component\n",
    "# Then calculate the diameter of the subgraph, just like you did with density.\n",
    "#\n",
    "\n",
    "subgraph = g.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-administrator",
   "metadata": {},
   "source": [
    "##### Triadic Closure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-budapest",
   "metadata": {},
   "source": [
    "Triadic closure supposes that if two people know the same person, they are likely to know each other. If Fox knows both Fell and Whitehead, then Fell and Whitehead may very well know each other, completing a **triangle** in the visualization of three edges connecting Fox, Fell, and Whitehead. The number of these enclosed triangles in the network can be used to find clusters and communities of individuals that all know each other fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-jewelry",
   "metadata": {},
   "source": [
    "One way of measuring triadic closure is called **clustering coefficient** because of this clustering tendency, but the structural network measure you will learn is known as **transitivity**.11 Transitivity is the ratio of all triangles over all possible triangles. A possible triangle exists when one person (Fox) knows two people (Fell and Whitehead). So transitivity, like density, expresses how interconnected a graph is in terms of a ratio of actual over possible connections. Remember, measurements like transitivity and density concern likelihoods rather than certainties. All the outputs of your Python script must be interpreted, like any other object of research. Transitivity allows you a way of thinking about all the relationships in your graph that may exist but currently do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "triadic_closure = nx.transitivity(g)\n",
    "print(f\"Triadic closure: {round(triadic_closure, 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-testament",
   "metadata": {},
   "source": [
    "Also like density, transitivity is scaled from 0 to 1, and you can see that the network’s transitivity is about 0.1694, somewhat higher than its 0.0248 density. Because the graph is not very dense, there are fewer possible triangles to begin with, which may result in slightly higher transitivity. That is, nodes that already have lots of connections are likely to be part of these enclosed triangles. To back this up, you’ll want to know more about nodes with many connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-noise",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-chassis",
   "metadata": {},
   "source": [
    "\n",
    "In network analysis, measures of the importance of nodes are referred to as **centrality** measures. Because there are many ways of approaching the question “Which nodes are the most important?” there are many different ways of calculating centrality. Here you’ll learn about three of the most common centrality measures: degree, betweenness centrality, and eigenvector centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-macintosh",
   "metadata": {},
   "source": [
    "##### Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-entrance",
   "metadata": {},
   "source": [
    "Degree is the simplest and the most common way of finding important nodes. A node’s degree is the sum of its edges. If a node has three lines extending from it to other nodes, its degree is three. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-mortgage",
   "metadata": {},
   "source": [
    "The nodes with the highest degree in a social network are the people who know the most people. These nodes are often referred to as hubs, and calculating degree is the quickest way of identifying hubs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-universe",
   "metadata": {},
   "source": [
    "All of the centrality commands you’ll learn in this section produce dictionaries in which the keys are nodes and the values are centrality measures. That means they’re ready-made to add back into your network as a node attribute, like you did in the last section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(g.degree(g.nodes()))\n",
    "nx.set_node_attributes(g, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.nodes['William Penn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-concern",
   "metadata": {},
   "source": [
    "In this case almost all of the hubs are founders of the religion or otherwise important political figures. Thankfully there are other centrality measures that can tell you about more than just hubs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-teens",
   "metadata": {},
   "source": [
    "##### Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-lexington",
   "metadata": {},
   "source": [
    "Eigenvector centrality is a kind of extension of degree—it looks at a combination of a node’s edges and the edges of that node’s neighbors. Eigenvector centrality cares if you are a hub, but it also cares how many hubs you are connected to. It’s calculated as a value from 0 to 1: the closer to one, the greater the centrality. Eigenvector centrality is useful for understanding which nodes can get information to many other nodes quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-tournament",
   "metadata": {},
   "source": [
    "##### Betweenness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-absorption",
   "metadata": {},
   "source": [
    "Betweenness centrality is a bit different from the other two measures in that it doesn’t care about the number of edges any one node or set of nodes has. Betweenness centrality looks at all **the shortest paths that pass through a particular node** (see above). To do this, it must first calculate every possible shortest path in your network, so keep in mind that betweenness centrality will take longer to calculate than other centrality measures (but it won’t be an issue in a dataset of this size). Betweenness centrality, which is also expressed on a scale of 0 to 1, is fairly good at finding nodes that connect two otherwise disparate parts of a network. If you’re the only thing connecting two clusters, every communication between those clusters has to pass through you. In contrast to a hub, this sort of node is often referred to as a broker. Betweenness centrality is not the only way of finding brokerage (and other methods are more systematic), but it’s a quick way of giving you a sense of which nodes are important not because they have lots of connections themselves but because they stand between groups, giving the network connectivity and cohesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(g) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(g) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(g, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(g, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(eigenvector_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by eigenvector centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-cocktail",
   "metadata": {},
   "source": [
    "What if you want to know which of the high betweenness centrality nodes had low degree? That is to say: which high-betweenness nodes are unexpected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-secret",
   "metadata": {},
   "source": [
    "You can confirm from these results that some people, like Leavens and Penington, have high betweenness centrality but low degree. This could mean that these women were important brokers, connecting otherwise disparate parts of the graph. You can also learn unexpected things about people you already know about—in this list you can see that Penn has lower degree than Quaker founder George Fox, but higher betweenness centrality. That is to say, simply knowing more people isn’t everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-paper",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-fabric",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
